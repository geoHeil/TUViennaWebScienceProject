trump tweets
========================================================
author: web science
date: 17.01.2016
autosize: true

Mention follower (bimodal)
========================================================

![mention follower](slides-figure/trumpBig.jpg)

interaction (actor)
========================================================

![mention follower](slides-figure/bimdalBig.jpg)

ego network
========================================================

![mention follower](slides-figure/egoBig.jpg)


Trumps wordcloud
========================================================
```{r, message=FALSE, warning=FALSE, include=FALSE}
wants <- c("SocialMediaLab", "magrittr", "igraph", "ggplot2", "wordcloud", "tm", "stringr", "lubridate", "scales", "dplyr", "purrr", "tidyr", "readr", "tidytext")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
lapply(wants, library, character.only=T)
```

```{r, echo=FALSE}
tweets <- read.csv("data/trumpTweets.csv")
nohandles <- str_replace_all(tweets$text, "@\\w+", "") 
wordCorpus <- Corpus(VectorSource(nohandles))
wordCorpus <- tm_map(wordCorpus,
          content_transformer(function(x) iconv(x,
              to='UTF-8',
              sub='byte')),
          mc.cores=1)
wordCorpus <- tm_map(wordCorpus,
                     removePunctuation
                     ,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     content_transformer(tolower),
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     removeWords,
                     stopwords("english"),
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     removeNumbers,
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     stripWhitespace,
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     removeWords,
  c("trump","realdonaldtrump","the",
    "like","http","https","httpst"))
pal <- brewer.pal(9,"Reds")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = wordCorpus, scale=c(6,0.1),
          max.words=250,
          random.order=FALSE,
          rot.per=0.35,
          use.r.layout=FALSE,
          colors=pal)
```

#trump hashtag wordcloud
========================================================

```{r, echo=FALSE}
tweets <- read.csv("data/trumpHashtagTweets.csv")
nohandles <- str_replace_all(tweets$text, "@\\w+", "") 
wordCorpus <- Corpus(VectorSource(nohandles))
wordCorpus <- tm_map(wordCorpus,
  content_transformer(function(x) iconv(x,
         to='UTF-8',
         sub='byte')),
  mc.cores=1)
wordCorpus <- tm_map(wordCorpus,
                     removePunctuation,
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     content_transformer(tolower),
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     removeWords,
                     stopwords("english"),
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     removeNumbers,
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     stripWhitespace,
                     lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
                     removeWords,
  c("trump","realdonaldtrump","the",
    "like","http","https","httpst"))
pal <- brewer.pal(9,"Reds")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = wordCorpus,
          scale=c(6,0.1),
          max.words=250,
          random.order=FALSE, 
	       rot.per=0.35,
	       use.r.layout=FALSE,
	       colors=pal)
```

device analysis androdid vs iOS: when is tweeted
========================================================


```{r, echo=FALSE}
tweets3 <- read.csv("data/trumpTweets.csv")
tweetsSmall <- tweets3 %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))
tweetsSmall$created <- ymd_hms(tweetsSmall$created)

tweetsSmall %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```
device analysis androdid vs iOS: tweets containing picture
========================================================


```{r, echo=FALSE}
tweet_picture_counts <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  count(source,
        picture = ifelse(str_detect(text, "t.co"),
          "Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "")

```
device analysis androdid vs iOS: tweets with quotation marks
========================================================


```{r, echo=FALSE}
tweetsSmall %>%
  count(source,
        quoted = ifelse(str_detect(text,
                 '^"'), "Quoted", "Not quoted")) %>%
  ggplot(aes(source, n, fill = quoted)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "") +
  ggtitle('Whether tweets start with a quotation mark (")')

```
device analysis androdid vs iOS: frequent words
========================================================


```{r, echo=FALSE}
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text,
       "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
android_iphone_ratios <- tweet_words %>%
  count(word, source) %>%
  filter(sum(n) >= 5) %>%
  spread(source, n, fill = 0) %>%
  ungroup() %>%
  mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
  mutate(logratio = log2(Android / iPhone)) %>%
  arrange(desc(logratio))

android_iphone_ratios %>%
  group_by(logratio > 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Android / iPhone log ratio") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))


```

number of tweets over time
========================================================

```{r, echo=FALSE}
tweets <- read.csv("data/trumpTweets.csv")
tweets$created <- ymd_hms(tweets$created)
ggplot(data = tweets, aes(x = created)) +
        geom_histogram(aes(fill = ..count..)) +
        theme(legend.position = "none") +
        xlab("Time") + ylab("Number of tweets") + 
        scale_fill_gradient(low = "midnightblue",
                            high = "aquamarine4")
```

wordcloud per subgroup
========================================================
```{r, echo=FALSE}
load('tweets.Rdata') # will overwrite tweets variable
g_twitter_actor <- tweets %>% Create("Actor", writeToFile=FALSE)

# Number and sizes of connected components
c <- igraph::components(g_twitter_actor, mode = 'weak')
c$no
summary(c$csize)
sort(c$csize, decreasing = T)[1:10]

makeWordcloud <- function(t){
  nohandles <- str_replace_all(t, "@\\w+", "") 
  wordCorpus <- Corpus(VectorSource(nohandles))
  wordCorpus <- tm_map(wordCorpus,
                       content_transformer(function(x) iconv(x,
                                                             to='UTF-8',
                                                             sub='byte')),
                       mc.cores=1)
  wordCorpus <- tm_map(wordCorpus,
                       removePunctuation,
                       lazy=TRUE)
  wordCorpus <- tm_map(wordCorpus,
                       content_transformer(tolower),
                       lazy=TRUE)
  wordCorpus <- tm_map(wordCorpus,
                       removeWords,
                       stopwords("english"),
                       lazy=TRUE)
  wordCorpus <- tm_map(wordCorpus,
                       removeNumbers,
                       lazy=TRUE)
  wordCorpus <- tm_map(wordCorpus,
                       stripWhitespace,
                       lazy=TRUE)
  wordCorpus <- tm_map(wordCorpus,
                       removeWords,
                       c("trump","realdonaldtrump","the",
                         "like","http","https","httpst"))
  pal <- brewer.pal(9,"Reds")
  pal <- pal[-(1:4)]
  set.seed(123)
  wordcloud(words = wordCorpus,
            scale=c(6,0.1),
            max.words=250,
            random.order=FALSE, 
            rot.per=0.35,
            use.r.layout=FALSE,
            colors=pal)
}

#http://stackoverflow.com/questions/41661096/subgraph-text-analysis-in-r-igraph/41664330#41664330
#unique(E(g_twitter_actor)$edgeType)
#library(reshape2); library(dplyr)
#Make data frame for retweets, mentions, replies
rts <- tweets %>% filter(!is.na(retweet_from))
ms <- tweets %>% filter(users_mentioned!="character(0)")
rpls <- tweets %>% filter(!is.na(reply_to))

#Name each element in the users_mentioned list after the user who mentioned
names(ms$users_mentioned) <- ms$screen_name
ms <- melt(ms$users_mentioned) #melting creates a data frame for each user and the users they mention

#Add the text
ms$text <- tweets[match(ms$L1,tweets$screen_name),1]

E(g_twitter_actor)$text[E(g_twitter_actor)$edgeType %in% "Retweet"] <- rts$text
E(g_twitter_actor)$text[E(g_twitter_actor)$edgeType %in% "Mention"] <- ms$text
E(g_twitter_actor)$text[E(g_twitter_actor)$edgeType %in% "Reply"] <- rpls$text

# this creates a sub cluster
# and just execute the wordcount code
# subCluster <- induced.subgraph(g_twitter_actor, V(g_twitter_actor)[which(c$membership == which.max(c$csize))])
subCluster1 <- induced.subgraph(g_twitter_actor, V(g_twitter_actor)[which(c$membership %in% order(c$csize, decreasing = TRUE)[1])])
subCluster2 <- induced.subgraph(g_twitter_actor, V(g_twitter_actor)[which(c$membership %in% order(c$csize, decreasing = TRUE)[2])])
subCluster3 <- induced.subgraph(g_twitter_actor, V(g_twitter_actor)[which(c$membership %in% order(c$csize, decreasing = TRUE)[3])])
makeWordcloud(E(subCluster1)$text)
makeWordcloud(E(subCluster2)$text)
makeWordcloud(E(subCluster3)$text)
```


how the data was collected
========================================================

- socialMediaLab package


reproducible research
========================================================

- latex, knitR, rMarkdown
