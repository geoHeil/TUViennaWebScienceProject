\documentclass[11pt]{report}

\usepackage{todonotes}
\usepackage{hyperref}
\begin{document}
\title{Web Science TU Vienna graph analysis project}
\author{Georg Heiler 1225063 | Jasmina Kadic 1620747}
\maketitle
\tableofcontents

\begin{abstract}
	We performed graph analysis of partial twitter network data obtained by hashtag \textbf{trump}. 
	Basic garph metrics are computed for two points in time.
	The network is visualized and we perform a text analysis per group. Trying to identify the real trump we check for posts from ios vs. android devices.\todo{TODO}
\end{abstract}

\chapter{Data Analysis and visualisation}
<<librariesLoading, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=FALSE, message=FALSE>>=
wants <- c("SocialMediaLab", "magrittr", "igraph", "ggplot2", "wordcloud", "tm", "stringr", "lubridate", "scales", "dplyr", "purrr", "tidyr", "readr", "tidytext")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
lapply(wants, library, character.only=T)
@
\section{Overview}
We chose to analyse a partial network of twitter with the hashtag \textbf{trump}.
The data was collected at two points in time. Once early December, the second time early January. Both times around 18000 tweets were collected (which represent the maximum number of tweets which can be retrieved at once before hitting the API limit).

The \hyperref{https://github.com/vosonlab/SocialMediaLab}{socialMediaLab} package to collect the data.
It supports to generate several types of networks for our kind of data

\begin{itemize}
  \item \textbf{bimodal} A user can comment or like a post. Therefor if user $i$ has liked and commented on a post two directed edges are created. The generated network is directed, weighted (\#ofComments), bipartite and contains multiple /parallel edges
  
  \item \textbf{actor} Interactions (mention, reply, retweet) between twitter users show who is interacting with whom in relation to a particular hashtag
  
  \item \textbf{semantic} Nodes represent unique concepts (words) extracted from the set of tweets, edges represent the cooccurence of terms for all observations.
  
\end{itemize}

Ego networks only work well out of the box with Instagram, dynamic networks with facebook. Semantic networks should work for this type of data source, however do not work for our data.

We will compare these 3 networks for some metrics.

\section{Network statistics and communities}

Basic statistics of the network are calculated in this section.
Additionally, communities  and centrality metrics are analyzed.

First we load the twitter graph data and create a graph.

<<label="0keep_metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
load('tweets.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

Here is the computation of the metrics. We define the following function to compute them:

<<label="metricsNumberVertices", results='markup', cache=TRUE>>=
calcStats <- function(graphName){
  # "summary" prints the number of vertices,
  #edges and whether the graph is directed: 
  print("Summary")
  summary(graphName)  
  
  # Number of nodes and edges 
  print(paste("Number of vertices:", vcount(graphName)))
  print(paste("Number of edges:", ecount(graphName)))
  print(paste("Graph is directed", is_directed(graphName)))
  print(paste("Edge density:", edge_density(graphName)))
  print(paste("Average distance:",
              mean_distance(graphName, directed=T)))
  print(paste("Diameter:", diameter(graphName, directed=T)))
  print(paste("Reciprocity: ",
              reciprocity(graphName, ignore.loops =TRUE,
                          mode = c("default", "ratio"))))
  
  # Degree distribution (in-degree,out-degree)
  deg <- igraph::degree(graphName)
  print(paste("Average degree:", mean(deg) ))
  deg_in <- igraph::degree(graphName, mode = "in")
  deg_out <- igraph::degree(graphName, mode = "out")
  print(paste("Average in-degree:",mean(deg_in) ))
  print(paste("Average out-degree:", mean(deg_out)))
  
  print("In-degree centrality (from highest to lowest)")
  print(sort(deg_in, decreasing = T)[1:10])
  print("Out-degree centrality (from highest to lowest)")
  print(sort(deg_out, decreasing = T)[1:10])
  
  # Eigenvector centrality
  eig <- eigen_centrality(graphName, directed=T)$vector
  print("Eigenvector centrality (from highest to lowest)")
  print(sort(eig, decreasing = T)[1:10] )
  
  #  Closeness centrality
  clos_in <- igraph::closeness(graphName, mode = "in") 
  print("In-closeness centrality (from highest to lowest)")
  print(sort(clos_in, decreasing = T)[1:10])
  clos_out <- igraph::closeness(graphName, mode = "out") 
  print("Out-closeness centrality (from highest to lowest)")
  print(sort(clos_out, decreasing = T)[1:10])

  # Betweeness centrality
  betw <- igraph::betweenness(graphName, directed=T) 
  print("Betweeness centrality (from highest to lowest)")
  print(sort(betw, decreasing = T)[1:10])

  # Hubs and Authorities
  hs <- hub_score(graphName)$vector
  print("Hub scores(from highest to lowest)")
  print(sort(hs, decreasing = T)[1:10])

  as <- authority_score(graphName)$vector
  print("Authoritie scores(from highest to lowest)")
  print(sort(as, decreasing = T)[1:10])

  # PageRank
  pr<-page_rank(graphName, algo =
                  c("prpack", "arpack", "power"),
                vids = V(graphName),
              directed = TRUE,
              damping = 0.85,
              personalized = NULL,
              weights = NULL,
              options = NULL)$vector
  print("Page rank (from highest to lowest)")
  print(sort(pr, decreasing = T)[1:10])

  # Number and sizes of connected components
  print(paste("Number of connected components:",
              igraph::components(graphName)$no))
  print("Sizes of connected components:")
  print(summary(igraph::components(graphName)$csize))
  # unfortunately graph will not show up in function
  # qplot(igraph::components(graphName)$csize,
  # geom="histogram", binwidth = 100,
  # main="Sizes of connected components")

  # Transitivity measures the probability
  # that the adjacent vertices of a vertex are connected.
  # This is sometimes also called the clustering coefficient. 
  print("Clustering coefficient (type=local)")
  clcoef <- transitivity(graphName, type = "local")
  print(tail(table(clcoef)))
  print(paste("Clustering coefficient, (type=global):",
              transitivity(graphName, type = "global"))) 
  print(paste("Clustering coefficient, (type=average):",
              transitivity(graphName, type = "average"))) 
}
@

For the actor graph statistics are

<<label="metricsNumberVerticesActor", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

For the bimodal graph statistics are
<<label="metricsNumberVerticesBimodal", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@

\todo{rework this summary and include reference to bimodal vs actor network}
From the results it is noticeable  that 
bimodal network is bigger and better connected. Diameter and average distance between nodes are significantly smaller in bimodal network.
Degree centrality counts hot many direct connections each node has to other nodes within the network, it is useful for finding individuals within the network who hold most informations, or who can connect with the wider network or are most popular.  Most influential nodes in network are calculated by in-degree centrality. These are profiles with most commented, mentioned or replied posts with most followers. It is interesting to see that Donald Trump is only fifth most  influential user in this data set. For finding nodes who are best placed to influence the entire network most quickly, closeness centrality is used. As expected Donald Trump is most influential in this network but interestingly enough he  does not have the  biggest score in eigenvector centrality. The node with most important edges is node with username \textit{funder}. This user has also biggest hub, authority and pagerank scores.  He has more than 45000 tweets and  majority are anti Trump tweets.

\section{Visualization}
Gephi was used for the visualization in Figure \ref{fig:plotActor}.
as well as for Figure \ref{fig:plotBimodal}.
We used the degree filter in gephi to filter the graph to a minimal degreee and to remove nodes with very few connections after the Giant Component filter to make sure to include only the main vertices.
The edges are coloured with the type of connection e.g. retweet, mention.

\begin{figure}[h]
  \centering
  \caption{Actor graph for trump tweets with edges coded as retweet and mention}
  \includegraphics[width=\textwidth]{slides-figure/g1actor1}
  \label{fig:plotActor}
\end{figure}
\begin{figure}[h]
  \centering
  \caption{Bimodal graph for trump tweets}
  \includegraphics[width=\textwidth]{slides-figure/g1bimodal1}
  \label{fig:plotBimodal}
\end{figure}

\chapter{Longitudinal}
How did the network change? What dynamics can be
identified? Are there critical events in the period observed?

The same metrics as before are computed now, but for a new dataset collected several weeks later.

loading of the new data
<<label="0keep_longLoading", results='markup', cache=TRUE>>=
load('tweets2.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

For the actoor
<<label="actorLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

and  for the bimodal version of the graph
<<label="bimodalLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@

\todo{check for critical events in that period maybe xmas / his new year?, she wanted us to specifically have something like that. we should at least make someting up}
Since data sets are gathered in close time periods and no important event has occurred in this period result are pretty much the same.

\section{ego network analysis}
We chose to analyze the ego network of donald trump. To perform the analysis gephi's ego network operator was used. The id of \@realDonaldTrump is n9830 even though about 5 similar accounts are involved we chose that one as the source for our ego network which was generated with an depth of 1. The result can be seen in igure \ref{fig:plotActorEgo}. Nodes are coloured by modularity class obtained from gephis statistical analysis capabilities. Additionally pagerank is visualized as the size of vertices.

Again edges were coded in red for retweets and green for mentions.

\begin{figure}[h]
  \centering
  \caption{Ego network of donald trump}
  \includegraphics[width=\textwidth]{slides-figure/geigen}
  \label{fig:plotActorEgo}
\end{figure}

\chapter{Text analysis}
What do the users talk about? Are there any keywords/topics that are particularly important? Does this vary in different groups/communities? Are the discussions controversial?

\todo{check this, add communities}
\todo{do not write about most frequent words but rather show the plots here}
Most frequent words on 06.01.2017 were: maga, will, trump, intelligence, hacking, news, briefing, usa, election, theresistance. 
After Meryl Streep had her speech on Golden Globes about Trump her mentions with hashtag Trump skyrocketed she is one of most frequently tweeted words on 10.01.2017 while searching twitter with trump hashtag. So even though there were no changes in 
metrics the content of tweets is different.

% strange chace=TRUE does not work
<<label="wordcloud", results='markup', cache=FALSE, warning=FALSE>>=
nohandles <- str_replace_all(tweets$text, "@\\w+", "") 
wordCorpus <- Corpus(VectorSource(nohandles))
wordCorpus <- tm_map(wordCorpus,
    content_transformer(function(x) iconv(x,
                        to='UTF-8',
                        sub='byte')),
                     mc.cores=1)
wordCorpus <- tm_map(wordCorpus,
      removePunctuation,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
      content_transformer(tolower),lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
      removeWords, stopwords("english"),lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
      removeNumbers,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
     stripWhitespace,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus,
     removeWords, c("trump","realdonaldtrump"))
wordCorpus <- tm_map(wordCorpus,
    removeWords, c("the", "https","httpst","like","one"))


tdm <- TermDocumentMatrix(wordCorpus,
        control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq,
                    term.freq >= 20)
df <- data.frame(term = names(term.freq),
                 freq = term.freq)
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m),
                  decreasing = T)
pal <- brewer.pal(9, "BuGn")[-(1:4)]

wordcloud(words = names(word.freq),
          freq = word.freq,
          min.freq = 3,
          random.order = F,
          colors = pal,
          max.words = 300)
@

We try to figure out if the real Trump is tweeting or one of his co-workers. Apparently only Trump is using an android device, most of the others are using iOS devices. 
We were following along with \url{http://varianceexplained.org/r/trump-tweets/} and adopted their approach for our data.
You can see the times when a tweet is posted form an android or ios device
<<terms123, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=TRUE, message=FALSE>>=
tweets3 <- read_csv("tweets3.csv")
@
<<label="terms", results='markup', cache=TRUE>>=
tweetsSmall <- tweets3 %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))

tweetsSmall %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")

@

Next, it is analyzed if the tweets are containing picture or not. As seen on graph is it more likely that  tweet with picture will be tweeted by Trumps staff. Compared with the original results from \url{http://varianceexplained.org/r/trump-tweets/} the reported gap between android and iOS has shrunk considerably.

<<label="terms2", results='markup', cache=TRUE>>=
tweet_picture_counts <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  count(source,
        picture = ifelse(str_detect(text, "t.co"),
                         "Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "")
@
Before election Trump had tendency to copy-paste other people tweets and then surround them with quotation marks. From our result it is visible he does not do that as much as before. Now only few quoted tweets are published across devices.
<<label="terms3", results='markup', cache=TRUE>>=
tweetsSmall %>%
  count(source,
        quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
  ggplot(aes(source, n, fill = quoted)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "") +
  ggtitle('Whether tweets start with a quotation mark (")')
@

Frequently tweeted words overall.

<<label="terms4", results='markup', cache=TRUE>>=
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

tweet_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip()
@

Frequently tweeted words by device type.

<<label="terms5", results='markup', cache=TRUE>>=
android_iphone_ratios <- tweet_words %>%
  count(word, source) %>%
  filter(sum(n) >= 5) %>%
  spread(source, n, fill = 0) %>%
  ungroup() %>%
  mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
  mutate(logratio = log2(Android / iPhone)) %>%
  arrange(desc(logratio))

android_iphone_ratios %>%
  group_by(logratio > 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Android / iPhone log ratio") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
@

   Results are showing that tweets from iPhone are using more hashtags while tweets from 
  Android include words moron and idiot. It is also noticeable that words from Android are much more frequently repeated.


As \url{http://varianceexplained.org/r/trump-tweets/} this could be accomplised by a sentiment analysis. But it is already pretty clear that some devices use a very different language.

\todo{}
 This graph shows how many times is it tweeted from Trumps twitter profile. It is noticeable that profile was more active before election and significantly decreased after election.

The number of tweets over time
<<label="textanalysis1", results='markup', cache=TRUE>>=
tweets <- read.csv("data/trumpTweets.csv")
tweets$created <- ymd_hms(tweets$created)
ggplot(data = tweets, aes(x = created)) +
        geom_histogram(aes(fill = ..count..)) +
        theme(legend.position = "none") +
        xlab("Time") + ylab("Number of tweets") + 
        scale_fill_gradient(low = "midnightblue",
                            high = "aquamarine4")
@

Wordcloud
\todo{add new wrdcloud code from trumTwitteR.r   126-146 line of code}
After filtering Trump tweets using libraries tm and wordcloud, wordcloud with mostly frequent used words is created. 


<<label="termsx", results='markup', cache=TRUE>>=
wordCorpus <- Corpus(VectorSource(tweets))
wordCorpus <- tm_map(wordCorpus, removePunctuation,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower),lazy=TRUE)

wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"),lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, removeNumbers,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, stripWhitespace,lazy=TRUE)

tdm <- TermDocumentMatrix(wordCorpus, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")[-(1:4)]

wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
@

\chapter{Discussion and conclusion}
What about the quality of the data? What are the limitations? What questions can be answered and what new insights can be gained by analyzing the structure of the networks? What nodes are in strategic locations? What can you tell about the communities identified? What do you learn by looking at the different networks? Why is it interesting to combine the information from structural and textual data? Interpret and discuss your findings!

The quality of the data is rather good. Without the `SocialMediaLab` package for R it would have been complex to generate the graph from the collected tweets. With  it this was an easy task and we were able to compute several "views" e.g. an actor and a bimodal graph from the same type of underlying data.

The twitter graph data contains a lot of columns
<<label="colnames", results='markup', cache=TRUE>>=
colnames(tweets3)
@
which specify when, what and how e.g. by which device something was tweeted, as well as the type of tweet (mention, retweet).


\todo{todo answer her questions}
Donald Trump is very controversial person and his success on presidential elections in USA in 2016 is still mystery to many people. He is nothing like any other president or presidential candidate in the past.  He does neither act or talk like a politician, rules for everyone else does not apply for him.  He had so many scandals, said so many things but he is still elected as the president of the United States of America. In his race he used social media but mostly Twitter to express his thought and feelings about various types of things. This all makes him very good topic for research in social media. There is so many data and papers written about this topic. We tried to analyze his behavior on Twitter after he is elected and also study Twitter partial network using hashtag trump.

  
First step in this project was to gather data. Among others, we used SocialMediaLab and twitteR packages, to collect data and store them. We collected data in different period of time. While gathering data we crossed on “rate limit exceeded” error messages, because Twitter has a limit on number of data that can be collected at once. After this we did a statistical analysis of the network. 
From the centrality measures results it is visible that a lot of data is from people who do not support Donald Trump. One specific node is very active in our network, with username funder, who has the biggest hub, authority,pagerank and degree centrality.

Because a lot of people are intrigued by Donald Trump and they already did analysis of his Twitter account even with Twitter limitations for collecting data, we could compare our results from when Donald Trump was presidential candidate and now when he is elected. This was mostly visible with text analysis because our network did not structurally changed much, and metrics stated the same, but textual data are changing daily. 

Most interesting part of textual analysis was to compare data of Trumps Twitter profile based on device it was Tweeted on. It is  known that Donald Trump uses Android phone and his staff uses iOS phones. From created graphs differences are  visually visible. It is noticeable from data that Trumps profile was more active before election. He used more copy-paste retweet method also before election while now he does not do that very frequently. It is also visible that Android tweets are more angrier with tendency to repeat same words more and ending tweets with description of the mood, while the iOS tweets tend to be announcements and contain more pictures.  

Donald Trump tweets like “normal” person would. He tweets about his views but also about irrelevant TV and movie oriented entertainment and not like a most influential person in the world. He has no filter but he has to learn that now his (typed) words  have great consequences.

You can delete last paragraph if you want.
\chapter{other things}
report (nearly complete)

code (included in report)

presentation \todo{prepare, I suggest sort of interactive mode}
\end{document}
