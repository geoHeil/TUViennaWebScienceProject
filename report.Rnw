\documentclass[11pt]{report}

\usepackage{todonotes}
\usepackage{hyperref}
\begin{document}
\title{Web Science TU Vienna graph analysis project}
\author{Georg Heiler 1225063 | Jasmina Kadic 1620747}
\maketitle
\tableofcontents

\begin{abstract}
	Network analysis of partial twitter network data obtained by hashtag \textbf{trump} is performed. 
	Basic garph metrics are computed for two points in time.
	The network is visualized and we perform text analysis for some selected groups. Trying to identify the real trump we check for posts from ios vs. android devices.
\end{abstract}

\chapter{Data Analysis and visualisation}
<<librariesLoading, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=FALSE, message=FALSE>>=
wants <- c("SocialMediaLab", "magrittr", "igraph", "ggplot2", "wordcloud", "tm", "stringr", "lubridate", "scales", "dplyr", "purrr", "tidyr", "readr", "tidytext")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
lapply(wants, library, character.only=T)
@
\section{Overview}
We chose to analyse a partial network of twitter with the hashtag \textbf{trump}.
The data was collected at two points in time. Once early December, the second time early January. Both times around 18000 tweets were collected. This number represent the maximum number of tweets which can be retrieved at once before hitting the API limit.

The socialMediaLab package was used to collect the data (\url{https://github.com/vosonlab/SocialMediaLab}).
It supports the generation of different types of networks:

\begin{itemize}
  \item \textbf{bimodal} A user can comment or like a post. Therefor if user $i$ has liked and commented on a post two directed edges are created. The generated network is directed, weighted (\#ofComments), bipartite and contains multiple /parallel edges
  
  \item \textbf{actor} Interactions (mention, reply, retweet) between twitter users show who is interacting with whom in relation to a particular hashtag
  
  \item \textbf{semantic} Nodes represent unique concepts (words) extracted from the set of tweets, edges represent the cooccurence of terms for all observations.
  
\end{itemize}

Ego networks only work well out of the box with Instagram, dynamic networks with facebook. Semantic networks should work for this type of data source, however do not work for our data.

We will compare these 3 networks for some metrics.

\section{Network statistics and communities}

Basic statistics of the network are calculated in this section.
Additionally, communities  and centrality metrics are analyzed.

First we load the twitter graph data and create a graph.

<<label="0keep_metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
load('tweets.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

Here is the computation of the metrics. We define the following function to compute them:

<<label="metricsNumberVertices", results='markup', cache=TRUE>>=
calcStats <- function(graphName){
  # "summary" prints the number of vertices,
  #edges and whether the graph is directed: 
  print("Summary")
  summary(graphName)  
  
  # Number of nodes and edges 
  print(paste("Number of vertices:", vcount(graphName)))
  print(paste("Number of edges:", ecount(graphName)))
  print(paste("Graph is directed", is_directed(graphName)))
  print(paste("Edge density:", edge_density(graphName)))
  print(paste("Average distance:",
              mean_distance(graphName, directed=T)))
  print(paste("Diameter:", diameter(graphName, directed=T)))
  print(paste("Reciprocity: ",
              reciprocity(graphName, ignore.loops =TRUE,
                          mode = c("default", "ratio"))))
  
  # Degree distribution (in-degree,out-degree)
  deg <- igraph::degree(graphName)
  print(paste("Average degree:", mean(deg) ))
  deg_in <- igraph::degree(graphName, mode = "in")
  deg_out <- igraph::degree(graphName, mode = "out")
  print(paste("Average in-degree:",mean(deg_in) ))
  print(paste("Average out-degree:", mean(deg_out)))
  
  print("In-degree centrality (from highest to lowest)")
  print(sort(deg_in, decreasing = T)[1:10])
  print("Out-degree centrality (from highest to lowest)")
  print(sort(deg_out, decreasing = T)[1:10])
  
  # Eigenvector centrality
  eig <- eigen_centrality(graphName, directed=T)$vector
  print("Eigenvector centrality (from highest to lowest)")
  print(sort(eig, decreasing = T)[1:10] )
  
  #  Closeness centrality
  clos_in <- igraph::closeness(graphName, mode = "in") 
  print("In-closeness centrality (from highest to lowest)")
  print(sort(clos_in, decreasing = T)[1:10])
  clos_out <- igraph::closeness(graphName, mode = "out") 
  print("Out-closeness centrality (from highest to lowest)")
  print(sort(clos_out, decreasing = T)[1:10])

  # Betweeness centrality
  betw <- igraph::betweenness(graphName, directed=T) 
  print("Betweeness centrality (from highest to lowest)")
  print(sort(betw, decreasing = T)[1:10])

  # Hubs and Authorities
  hs <- hub_score(graphName)$vector
  print("Hub scores(from highest to lowest)")
  print(sort(hs, decreasing = T)[1:10])

  as <- authority_score(graphName)$vector
  print("Authoritie scores(from highest to lowest)")
  print(sort(as, decreasing = T)[1:10])

  # PageRank
  pr<-page_rank(graphName, algo =
                  c("prpack", "arpack", "power"),
                vids = V(graphName),
              directed = TRUE,
              damping = 0.85,
              personalized = NULL,
              weights = NULL,
              options = NULL)$vector
  print("Page rank (from highest to lowest)")
  print(sort(pr, decreasing = T)[1:10])

  # Number and sizes of connected components
  print(paste("Number of connected components:",
              igraph::components(graphName)$no))
  print("Sizes of connected components:")
  print(summary(igraph::components(graphName)$csize))
  # unfortunately graph will not show up in function
  # qplot(igraph::components(graphName)$csize,
  # geom="histogram", binwidth = 100,
  # main="Sizes of connected components")

  # Transitivity measures the probability
  # that the adjacent vertices of a vertex are connected.
  # This is sometimes also called the clustering coefficient. 
  print("Clustering coefficient (type=local)")
  clcoef <- transitivity(graphName, type = "local")
  print(tail(table(clcoef)))
  print(paste("Clustering coefficient, (type=global):",
              transitivity(graphName, type = "global"))) 
  print(paste("Clustering coefficient, (type=average):",
              transitivity(graphName, type = "average"))) 
}
@

For the actor graph statistics are

<<label="metricsNumberVerticesActor", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

For the bimodal graph statistics are
<<label="metricsNumberVerticesBimodal", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@


From the results it is noticeable  that the 
bimodal network is bigger and better connected.

<<label="ggmetricsNumberVerticesSlow", results='markup', cache=TRUE>>=
	 g1 <- read.graph("actorNetwork.graphml", format = "graphml")
 	 g2 <- read.graph("bimodalNetwork.graphml", format = "graphml")
 	 print(paste("Number of vertices of actor network:", vcount(g1)))
	 print(paste("Number of edges of actor network:", ecount(g1)))
	 print(paste("Number of vertices of bimodal network:", vcount(g2)))
	 print(paste("Number of edges of bimodal network:", ecount(g2)))
@
 
To calculate the shortest path in the network function mean_distance is used, and for calculating longest path between two nodes in the network we used function diameter.

<<label="metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
  print(paste("Average distance of actor network:",mean_distance(g1, directed=T)))
  print(paste("Average distance of bimodal network:",mean_distance(g2, directed=T)))
  print(paste("Diameter of actor network:", diameter(g1, directed=T)))
  print(paste("Diameter of bimodal network:", diameter(g2, directed=T)))
@


Degree centrality counts how many direct connections each node has to other nodes within the network. This is useful for finding individuals within the network who hold most informations, or who can connect with the wider network or are most popular.  Most influential nodes in network are calculated by in-degree centrality. In actor network 
most influential nodes are:

<<label="1metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
  deg <- igraph::degree(g1)
  deg_in <- igraph::degree(g1, mode = "in")
  print("In-degree centrality (from highest to lowest)")
  print(sort(deg_in, decreasing = T)[1:10])
@
These are profiles with most commented, mentioned or replied posts with most followers. It is interesting to see that Donald Trump is only fifth most  influential user in this data set. 

Nodes with biggest in-degree centrality in bimodal network are:

<<label="2metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
  deg <- igraph::degree(g2)
  deg_in <- igraph::degree(g2, mode = "in")
  print("In-degree centrality (from highest to lowest)")
  print(sort(deg_in, decreasing = T)[1:10])
@

To find which nodes are best placed to influence the entire network most quickly, closeness centrality is used. In actor network nodes with the biggest in-closeness centrality are:

<<label="3metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
	clos_in <- igraph::closeness(g1, mode = "in") 
    print("In-closeness centrality (from highest to lowest)")
    print(sort(clos_in, decreasing = T)[1:10])
@

 In bimodal network:

<<label="4metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
	clos_in <- igraph::closeness(g2, mode = "in") 
    print("In-closeness centrality (from highest to lowest)")
    print(sort(clos_in, decreasing = T)[1:10])
@
 

As expected Donald Trump is most influential in this network but interestingly enough he does not have the  biggest score in eigenvector centrality. 

<<label="5metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
  eig <- eigen_centrality(g1, directed=T)$vector
  print("Eigenvector centrality (from highest to lowest)")
  print(sort(eig, decreasing = T)[1:10])
@

While calculating eigenvector centrality some nodes are more relative than others, and, reasonably, endorstments from important nodes count more. Eigenvector centrality definition sayst that a node is important if it is linked to by other important nodes.
The node with most important edges, with biggest eigenvector centrality score, is node with username \textit{funder}. This user has also biggest hub, authority and pagerank scores.  He has more than 45000 tweets and  majority are anti Trump tweets.

\section{Visualization}
Gephi was used for the visualization in Figure \ref{fig:plotActor}.
as well as for Figure \ref{fig:plotBimodal}.
We used the degree filter in gephi to filter the graph to a minimal degreee and to remove nodes with very few connections after the Giant Component filter to make sure to include only the main vertices.
The edges are coloured with the type of connection e.g. retweet, mention.

\begin{figure}[h]
  \centering
  \caption{Actor graph for trump tweets with edges coded as retweet and mention}
  \includegraphics[width=\textwidth]{slides-figure/g1actor1}
  \label{fig:plotActor}
\end{figure}
\begin{figure}[h]
  \centering
  \caption{Bimodal graph for trump tweets}
  \includegraphics[width=\textwidth]{slides-figure/g1bimodal1}
  \label{fig:plotBimodal}
\end{figure}

\chapter{Longitudinal}
The same metrics as before are computed now, but for a new dataset collected several weeks later.

loading of the new data
<<label="0keep_longLoading", results='markup', cache=TRUE>>=
load('tweets2.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

For the actoor
<<label="actorLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

and  for the bimodal version of the graph
<<label="bimodalLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@

\todo{Metrics are identical. Fell free to make something up. Go nuts :D}
Since data sets are gathered in close time periods and no important event has occurred in this period result are pretty much the same.

\section{ego network analysis}
We chose to analyze the ego network of Donald Trump. To perform the analysis gephi's ego network operator was used. The id of \emph{realDonaldTrump} is n9830.
Though about 5 similar accounts are involved we chose that one as the source for our ego network which was generated with an depth of 1. The result can be seen in Figure \ref{fig:plotActorEgo}. Nodes are coloured by modularity class obtained from gephis statistical analysis capabilities. Additionally pagerank is visualized as the size of vertices.

Again edges were coded in red for retweets and green for mentions.

\begin{figure}[h]
  \centering
  \caption{Ego network of Donald Trump}
  \includegraphics[width=\textwidth]{slides-figure/geigen}
  \label{fig:plotActorEgo}
\end{figure}

\chapter{Text analysis}
	For visualization of the most frequent words, contained in tweets, libraries wordcloud and tm are used. Wordcloud is relatively simple to make but it is important to do some text manipulation first (cleaning and formating tweets). 

	Wordcloud with mostly frequent words from Donald Trump:

	<<label="wordcloud1", results='markup', cache=FALSE, warning=FALSE>>=
	tweets <- read.csv("data/trumpTweets.csv")
	nohandles <- str_replace_all(tweets$text, "@\\w+", "") 
	wordCorpus <- Corpus(VectorSource(nohandles))
	wordCorpus <- tm_map(wordCorpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),mc.cores=1)

	wordCorpus <- tm_map(wordCorpus, removePunctuation,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, content_transformer(tolower),lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"),lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeNumbers,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, stripWhitespace,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeWords, c("trump","realdonaldtrump","the","like","http","https","httpst"))


	pal <- brewer.pal(9,"Reds")
	pal <- pal[-(1:4)]
	set.seed(123)

	wordcloud(words = wordCorpus, scale=c(6,0.1), max.words=250, random.order=FALSE, 
	          rot.per=0.35, use.r.layout=FALSE, colors=pal)

	@

	Most frequent words can also be ploted using TermDocumentMatrix. For Trump tweets graph is given below.

	<<label="wordcloud", results='markup', cache=FALSE, warning=FALSE>>=
		tdm <- TermDocumentMatrix(wordCorpus, control = list(wordLengths = c(1, Inf)))
	(freq.terms <- findFreqTerms(tdm, lowfreq = 20))
	term.freq <- rowSums(as.matrix(tdm))
	term.freq <- subset(term.freq, term.freq >= 20)
	df <- data.frame(term = names(term.freq), freq = term.freq)
	ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip() 
@
	Next it would be interesting to see how does a wordcloud looks like with data that contain hashtag trump. This data set contains 10 000 tweets but we are showing only 250 most frequent words.
	<<label="wordcloud2", results='markup', cache=FALSE, warning=FALSE>>=
	tweets <- read.csv("data/trumpHashtagTweets.csv")
	nohandles <- str_replace_all(tweets$text, "@\\w+", "") 
	wordCorpus <- Corpus(VectorSource(nohandles))
	wordCorpus <- tm_map(wordCorpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),mc.cores=1)

	wordCorpus <- tm_map(wordCorpus, removePunctuation,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, content_transformer(tolower),lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"),lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeNumbers,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, stripWhitespace,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeWords, c("trump","realdonaldtrump","the","like","http","https","httpst"))


	pal <- brewer.pal(9,"Reds")
	pal <- pal[-(1:4)]
	set.seed(123)

	wordcloud(words = wordCorpus, scale=c(6,0.1), max.words=250, random.order=FALSE, 
	          rot.per=0.35, use.r.layout=FALSE, colors=pal)
	@

	After Meryl Streep had her speech on Golden Globes about Trump her mentions with hashtag trump skyrocketed she is one of most frequently tweeted words on 10.01.2017 while searching twitter with trump hashtag. So even though there were no changes in 
	metrics the content of tweets is different. This data set contains more that 18 000 tweets.

	<<label="wordcloud3", results='markup', cache=FALSE, warning=FALSE>>=
	tweets <- read.csv("tweets3.csv")
	nohandles <- str_replace_all(tweets$text, "@\\w+", "") 
	wordCorpus <- Corpus(VectorSource(nohandles))
	wordCorpus <- tm_map(wordCorpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),mc.cores=1)

	wordCorpus <- tm_map(wordCorpus, removePunctuation,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, content_transformer(tolower),lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"),lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeNumbers,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, stripWhitespace,lazy=TRUE)
	wordCorpus <- tm_map(wordCorpus, removeWords, c("trump","realdonaldtrump","the","like","http","https","httpst","httpstco","htt"))


	pal <- brewer.pal(9,"Reds")
	pal <- pal[-(1:4)]
	set.seed(123)

	wordcloud(words = wordCorpus, scale=c(6,0.1), max.words=250, random.order=FALSE, 
	          rot.per=0.35, use.r.layout=FALSE, colors=pal)

	@



We try to figure out if the real Trump is tweeting or one of his co-workers. Apparently only Trump is using an Android device, his staff are using iOS devices. 
We were following along with \url{http://varianceexplained.org/r/trump-tweets/} and adopted  approach for our data.
You can see the times when a tweet is posted from an Android or iOS device.

<<terms123, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=TRUE, message=FALSE>>=
tweets3 <- read.csv("data/trumpTweets.csv")
@
<<label="terms", results='markup', cache=TRUE>>=
tweetsSmall <- tweets3 %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))

tweetsSmall %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")

@

Next, it is analyzed if the tweets are containing picture or not. As seen on graph is it more likely that  tweet with picture will be tweeted by Trumps staff. Compared with the original results from \url{http://varianceexplained.org/r/trump-tweets/} the reported gap between android and iOS has shrunk considerably.

<<label="terms2", results='markup', cache=TRUE>>=
tweet_picture_counts <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  count(source,
        picture = ifelse(str_detect(text, "t.co"),
                         "Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "")
@
Before election Trump had tendency to copy-paste other people tweets and then surround them with quotation marks. From our result it is visible he does not do that as much as before. Now only few quoted tweets are published across devices.
<<label="terms3", results='markup', cache=TRUE>>=
tweetsSmall %>%
  count(source,
        quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
  ggplot(aes(source, n, fill = quoted)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "") +
  ggtitle('Whether tweets start with a quotation mark (")')
@

Frequently tweeted words overall.

<<label="terms4", results='markup', cache=TRUE>>=
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

tweet_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip()
@

Frequently tweeted words by device type.

<<label="terms5", results='markup', cache=TRUE>>=
android_iphone_ratios <- tweet_words %>%
  count(word, source) %>%
  filter(sum(n) >= 5) %>%
  spread(source, n, fill = 0) %>%
  ungroup() %>%
  mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
  mutate(logratio = log2(Android / iPhone)) %>%
  arrange(desc(logratio))

android_iphone_ratios %>%
  group_by(logratio > 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Android / iPhone log ratio") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
@

   Results are showing that tweets from iPhone are using more hashtags while tweets from 
  Android include words moron and idiot. It is also noticeable that words from Android are much more frequently repeated.

As \url{http://varianceexplained.org/r/trump-tweets/} this could be accomplised by a sentiment analysis. But it is already pretty clear that some devices use a very different language.


When analyzing the twitter-wall of trump instead we observed the following.
The graph shows how many times it is tweeted from Trumps twitter profile. It is noticeable that profile was more active before election and significantly decreased after election.

The number of tweets over time
<<label="textanalysis1", results='markup', cache=TRUE>>=
tweets <- read.csv("data/trumpTweets.csv")
tweets$created <- ymd_hms(tweets$created)
ggplot(data = tweets, aes(x = created)) +
        geom_histogram(aes(fill = ..count..)) +
        theme(legend.position = "none") +
        xlab("Time") + ylab("Number of tweets") + 
        scale_fill_gradient(low = "midnightblue",
                            high = "aquamarine4")
@


\chapter{Discussion and conclusion}
What about the quality of the data? What are the limitations? What questions can be answered and what new insights can be gained by analyzing the structure of the networks? What nodes are in strategic locations? What can you tell about the communities identified? What do you learn by looking at the different networks? Why is it interesting to combine the information from structural and textual data? Interpret and discuss your findings!


Donald Trump is very controversial person and his success on presidential elections in USA in 2016 is still mystery to many people. In his race he used social media but mostly Twitter to express his thought and feelings about various types of things. This all makes him very good topic for research in social media. There is so many data and papers written about this topic. We tried to analyze his behavior on Twitter after he is elected and also study Twitter partial network using hashtag trump.

  
First step in this project was to gather data. Among others, we used SocialMediaLab and twitteR packages, to collect data and store them. We collected data in different period of time. While gathering data we crossed on “rate limit exceeded” error messages, because Twitter has a limit on number of data that can be collected at once.The quality of the data is rather good. Without the `SocialMediaLab` package for R it would have been complex to generate the graph from the collected tweets. With  it this was an easy task and we were able to compute several "views" e.g. an actor and a bimodal graph from the same type of underlying data.

The twitter graph data contains a lot of columns
<<label="colnames2", results='markup', cache=TRUE>>=
colnames(tweets3)
@
which specify when, what and how e.g. by which device something was tweeted, as well as the type of tweet (mention, retweet).

After this we did a statistical analysis of the network. 
From the centrality measures results it is visible that a lot of data is from people who do not support Donald Trump. Next to realDonaldTrump one specific node is very active in our actor network, with username funder, who has the biggest hub, authority,pagerank, eigenvector and degree centrality.

<<label="colnames3", results='markup', cache=TRUE>>=
 g1 <- read.graph("actorNetwork.graphml", format = "graphml")
 g2 <- read.graph("bimodalNetwork.graphml", format = "graphml")
 eig <- eigen_centrality(g1, directed=T)$vector
  print("Most influential nodes (from highest to lowest)")
  print(sort(eig, decreasing = T)[1:10])
@

Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. Nodes that can connect biggest number of people are in actor network are:

<<label="colnames", results='markup', cache=TRUE>>=
 g1 <- read.graph("actorNetwork.graphml", format = "graphml")
 g2 <- read.graph("bimodalNetwork.graphml", format = "graphml")
  betw <- igraph::betweenness(g1, directed=T) 
  print("Betweeness centrality (from highest to lowest)")
  print(sort(betw, decreasing = T)[1:10])
@

Actor network has 1062 communities.

<<label="colnames4", results='markup', cache=TRUE>>=
 g1 <- read.graph("actorNetwork.graphml", format = "graphml")
 g2 <- read.graph("bimodalNetwork.graphml", format = "graphml")
c <- igraph::components(g1, mode = 'weak')
c$no
summary(c$csize)
sort(c$csize, decreasing = T)[1:10]
@


Because a lot of people are intrigued by Donald Trump and they already did analysis of his Twitter account even with Twitter limitations for collecting data, we could compare our results from when Donald Trump was presidential candidate and now when he is elected. This was mostly visible with text analysis because our network did not structurally changed much, and metrics stated the same, but textual data are changing daily. 

Most interesting part of textual analysis was to compare data of Trumps Twitter profile based on device it was Tweeted on. It is  known that Donald Trump uses Android phone and his staff uses iOS phones. From created graphs differences are  visually visible. It is noticeable from data that Trumps profile was more active before election. He used more copy-paste retweet method also before election while now he does not do that very frequently. It is also visible that Android tweets are more angrier with tendency to repeat same words more and ending tweets with description of the mood, while the iOS tweets tend to be announcements and contain more pictures.  


\chapter{other things}
report (nearly complete)

code (included in report)

presentation 
\end{document}
