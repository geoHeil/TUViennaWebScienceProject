\documentclass[11pt]{report}

\usepackage{todonotes}
\usepackage{hyperref}
\begin{document}
\title{Web Science TU Vienna graph analysis project}
\author{Georg Heiler 1225063 | Jasmina Kadic}
\maketitle
\tableofcontents

\begin{abstract}
	Graph analysis of partial twitter network obtained by hashtag \textbf{trump}. \todo{TODO}
\end{abstract}

\chapter{Data Analysis and visualisation}
<<librariesLoading, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=FALSE, message=FALSE>>=
wants <- c("SocialMediaLab", "magrittr", "igraph", "ggplot2", "wordcloud", "tm", "stringr", "lubridate", "scales", "dplyr", "purrr", "tidyr")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
lapply(wants, library, character.only=T)
@
\section{Overview}
We chose to analyse a partial network of twitter with the hashtag \textbf{trump}.
Our dataset contains \todo{how many records} records.
The data was collected at \todo{specify points in time}.
We chose to use the \hyperref{https://github.com/vosonlab/SocialMediaLab}{socialMediaLab} package to collect the data.
It supports to generate several types of networks for our kind of data

\begin{itemize}
  \item \textbf{bimodal} A user can comment or like a post. Therefor if user $i$ has liked and commented on a post two directed edges are created. The generated network is directed, weighted (\#ofComments), bipartite and contains multiple /parallel edges
  
  \item \textbf{actor} Interactions (mention, reply, retweet) between twitter users show who is interacting with whom in relation to a particular hashtag
  
  \item \textbf{semantic} Nodes represent unique concepts (words) extracted from the set of tweets, edges represent the cooccurence of terms for all observations.
  
\end{itemize}

Ego networks only work well out of the box with Instagram, dynamic networks with facebook. Semantic networks should work for this type of data source, however do not work for our data.

We will compare these 3 networks for some metrics.

\section{Network statistics and communities}

Basic statistics of the network are represented in this section.

Calculate different metrics and measures, e.g., number of vertices, number of edges and their weights, degree distribution (resp. in-degree and out-degree in the directed case), centrality indices, clustering coefficient, reciprocity, shortest paths, network diameter, density, number and size of connected components.

Additionally, communities  and centrality metrics are analyzed.

<<label="metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
load('tweets.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

And here the computation of the metrics. We define the following function to compute them:

<<label="metricsNumberVertices", results='markup', cache=TRUE>>=
calcStats <- function(graphName){
  # "summary" prints the number of vertices,
  #edges and whether the graph is directed: 
  print("Summary")
  summary(graphName)  
  
  # Number of nodes and edges 
  print(paste("Number of vertices:", vcount(graphName)))
  print(paste("Number of edges:", ecount(graphName)))
  print(paste("Graph is directed", is_directed(graphName)))
  print(paste("Edge density:", edge_density(graphName)))
  print(paste("Average distance:",
              mean_distance(graphName, directed=T)))
  print(paste("Diameter:", diameter(graphName, directed=T)))
  print(paste("Reciprocity: ",
              reciprocity(graphName, ignore.loops =TRUE,
                          mode = c("default", "ratio"))))
  
  # Degree distribution (in-degree,out-degree)
  deg <- igraph::degree(graphName)
  print(paste("Average degree:", mean(deg) ))
  deg_in <- igraph::degree(graphName, mode = "in")
  deg_out <- igraph::degree(graphName, mode = "out")
  print(paste("Average in-degree:",mean(deg_in) ))
  print(paste("Average out-degree:", mean(deg_out)))
  
  print("In-degree centrality (from highest to lowest)")
  print(sort(deg_in, decreasing = T)[1:10])
  print("Out-degree centrality (from highest to lowest)")
  print(sort(deg_out, decreasing = T)[1:10])
  
  # Eigenvector centrality
  eig <- eigen_centrality(graphName, directed=T)$vector
  print("Eigenvector centrality (from highest to lowest)")
  print(sort(eig, decreasing = T)[1:10] )
  
  #  Closeness centrality
  clos_in <- igraph::closeness(graphName, mode = "in") 
  print("In-closeness centrality (from highest to lowest)")
  print(sort(clos_in, decreasing = T)[1:10])
  clos_out <- igraph::closeness(graphName, mode = "out") 
  print("Out-closeness centrality (from highest to lowest)")
  print(sort(clos_out, decreasing = T)[1:10])

  # Betweeness centrality
  betw <- igraph::betweenness(graphName, directed=T) 
  print("Betweeness centrality (from highest to lowest)")
  print(sort(betw, decreasing = T)[1:10])

  # Hubs and Authorities
  hs <- hub_score(graphName)$vector
  print("Hub scores(from highest to lowest)")
  print(sort(hs, decreasing = T)[1:10])

  as <- authority_score(graphName)$vector
  print("Authoritie scores(from highest to lowest)")
  print(sort(as, decreasing = T)[1:10])

  # PageRank
  pr<-page_rank(graphName, algo =
                  c("prpack", "arpack", "power"),
                vids = V(graphName),
              directed = TRUE,
              damping = 0.85,
              personalized = NULL,
              weights = NULL,
              options = NULL)$vector
  print("Page rank (from highest to lowest)")
  print(sort(pr, decreasing = T)[1:10])

  # Number and sizes of connected components
  print(paste("Number of connected components:",
              igraph::components(graphName)$no))
  print("Sizes of connected components:")
  print(summary(igraph::components(graphName)$csize))
  # unfortunately graph will not show up in function
  # qplot(igraph::components(graphName)$csize,
  # geom="histogram", binwidth = 100,
  # main="Sizes of connected components")

  # Transitivity measures the probability
  # that the adjacent vertices of a vertex are connected.
  # This is sometimes also called the clustering coefficient. 
  print("Clustering coefficient (type=local)")
  clcoef <- transitivity(graphName, type = "local")
  print(tail(table(clcoef)))
  print(paste("Clustering coefficient, (type=global):",
              transitivity(graphName, type = "global"))) 
  print(paste("Clustering coefficient, (type=average):",
              transitivity(graphName, type = "average"))) 
}
@

For the actor graph statistics are

<<label="metricsNumberVerticesActor", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

For the bimodal graph statistics are
<<label="metricsNumberVerticesBimodal", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@

The bimodal graph contains a lot more edges ie. is bigger. \todo{summary}

\section{Visualization}
Use different layouts to find and highlight patterns and relationships. To make sense of the data more easily, map the metrics and measures calculated onto visual properties such as size, colour, and opacity.

Gephi was used for the visualization in Figure \ref{fig:plotActor}.
as well as for Figure \ref{fig:plotBimodal}.

\begin{figure}[h]
  \centering
  \caption{Actor graph for trump tweets with edges coded as retweet and mention}
  \includegraphics[width=\textwidth]{slides-figure/g1actor1}
  \label{fig:plotActor}
\end{figure}
\begin{figure}[h]
  \centering
  \caption{Bimodal graph for trump tweets}
  \includegraphics[width=\textwidth]{slides-figure/g1bimodal1}
  \label{fig:plotBimodal}
\end{figure}

\todo{more visualization and description of the plots}

\chapter{Longitudinal}
How did the network change? What dynamics can be
identified? Are there critical events in the period observed?

The same metrics as before are computed now, but for a new dataset collected several weeks later.
loading of the new data
<<label="longLoading", results='markup', cache=TRUE>>=
load('tweets2.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

For the actoor
<<label="actorLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

and  for the bimodal version of the graph
<<label="bimodalLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@


\todo{summary}
some summary

\section{ego network analysis}
\todo{ego network}

\chapter{Text analysis}
What do the users talk about? Are there any keywords/topics that are particularly important? Does this vary in different groups/communities? Are the discussions controversial?

<<label="textanalysis1", results='markup', cache=TRUE>>=
tweets <- read.csv("data/trumpTweets.csv")

ggplot(tweets, aes(factor(grepl("#",tweets$text)))) +
        geom_bar(fill = "midnightblue") + 
        theme(legend.position="none", axis.title.x = element_blank()) +
        ylab("Number of tweets") + 
        ggtitle("Tweets with Hashtags") +
        scale_x_discrete(labels=c("No hashtags", "Tweets with hashtags"))

ggplot(tweets, aes(factor(grepl('^"', tweets$text)))) +
        geom_bar(fill = "midnightblue") + 
        theme(legend.position="none", axis.title.x = element_blank()) +
        ylab("Number of tweets") + 
        ggtitle("Tweets with quatation mark") +
        scale_x_discrete(labels=c("No quatation mark", "Tweets with quatation mark"))

tweets$created <- ymd_hms(tweets$created)

ggplot(data = tweets, aes(x = created)) +
        geom_histogram(aes(fill = ..count..)) +
        theme(legend.position = "none") +
        xlab("Time") + ylab("Number of tweets") + 
        scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
@

Wordcloud
\todo{filter data more}

<<label="terms", results='markup', cache=TRUE>>=
wordCorpus <- Corpus(VectorSource(tweets))
wordCorpus <- tm_map(wordCorpus, removePunctuation,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower),lazy=TRUE)

wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"),lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, removeNumbers,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, stripWhitespace,lazy=TRUE)

tdm <- TermDocumentMatrix(wordCorpus, control = list(wordLengths = c(1, Inf)))
(freq.terms <- findFreqTerms(tdm, lowfreq = 20))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 20)
df <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip() 

m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")[-(1:4)]

wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
@

Analysis of devices

we try to figure out if the real Trump is tweeting or one of his co-workers. Apparently only Trump is using an android device, most of the others are using iOS devices.

\todo{describe}
TODOD http://varianceexplained.org/r/trump-tweets/ some more
<<label="wordcloudx", results='markup', cache=TRUE>>=
device <- function(dataframe) {
    tot_all <- length(dataframe$statusSource)
 
    Web <- 100*(length(grep("Twitter Web Client", dataframe$statusSource))/tot_all)
    TweetDeck <- 100*(length(grep("TweetDeck", dataframe$statusSource))/tot_all)
    iPhone <- 100*(length(grep("Twitter for iPhone", dataframe$statusSource))/tot_all)
    iPad <- 100*(length(grep("Twitter for iPad", dataframe$statusSource))/tot_all)
    Blackberry <- 100*(length(grep("Twitter for BlackBerry", dataframe$statusSource))/tot_all)
    Tweetbot <- 100*(length(grep("Tweetbot for i", dataframe$statusSource))/tot_all)
    Hootsuite <- 100*(length(grep("Hootsuite", dataframe$statusSource))/tot_all)
    Android <- 100*(length(grep("Twitter for Android", dataframe$statusSource))/tot_all)
    Ads <- 100*(length(grep("Twitter Ads", dataframe$statusSource))/tot_all)
    M5 <- 100*(length(grep("Mobile Web (M5)", dataframe$statusSource))/tot_all)
    Mac <- 100*(length(grep("Twitter for Mac", dataframe$statusSource))/tot_all)
    Facebook <- 100*(length(grep("Facebook", dataframe$statusSource))/tot_all)
    Instagram <- 100*(length(grep("Instagram", dataframe$statusSource))/tot_all)
    IFTT <- 100*(length(grep("IFTT", dataframe$statusSource))/tot_all)
    Buffer <- 100*(length(grep("Buffer", dataframe$statusSource))/tot_all)
    CoSchedule <- 100*(length(grep("CoSchedule", dataframe$statusSource))/tot_all)
    GainApp <- 100*(length(grep("Gain App", dataframe$statusSource))/tot_all)
    MobileWeb <- 100*(length(grep("Mobile Web", dataframe$statusSource))/tot_all)
    iOS <- 100*(length(grep("iOS", dataframe$statusSource))/tot_all)
    OSX <- 100*(length(grep("OS X", dataframe$statusSource))/tot_all)
    Echofon <- 100*(length(grep("Echofon", dataframe$statusSource))/tot_all)
    Fireside <- 100*(length(grep("Fireside Publishing", dataframe$statusSource))/tot_all)
    Google <- 100*(length(grep("Google", dataframe$statusSource))/tot_all)
    MailChimp <- 100*(length(grep("MailChimp", dataframe$statusSource))/tot_all)
    TwitterForWebsites <- 100*(length(grep("Twitter for Websites", dataframe$statusSource))/tot_all)
 
    percentages <- data.frame(Web, TweetDeck, iPhone, iPad, Blackberry, Tweetbot,
               Hootsuite, Android, Ads, M5, Mac, Facebook, Instagram,
               IFTT, Buffer, CoSchedule, GainApp, MobileWeb, iOS, OSX,
               Echofon, Fireside, Google, MailChimp, TwitterForWebsites)
    return(percentages) }

trumpDevice <- sort(device(tweets), decreasing = T)
 


iPhone <- length(grep("Twitter for iPhone", tweets$statusSource))
Android <- length(grep("Twitter for Android", tweets$statusSource))

iPhone
Android
@

\todo{description}

<<label="textanalysis2", results='markup', cache=TRUE>>=
ggplot(data = tweets, aes(x = wday(created))) +
        geom_histogram(breaks = seq(0.5, 8, by =1), aes(fill = ..count..)) +
        theme(legend.position = "none") +
        xlab("Day of the Week") + ylab("Number of tweets") + 
        scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
@

\chapter{Discussion and conclusion}
What about the quality of the data? What are the limitations? What questions can be answered and what new insights can be gained by analyzing the structure of the networks? What nodes are in strategic locations? What can you tell about the communities identified? What do you learn by looking at the different networks? Why is it interesting to combine the information from structural and textual data? Interpret and discuss your findings!

The quality of the data is rather good. Without the `SocialMediaLab` package for R it would have been complex to generate the graph from the collected tweets. With  it this was an easy task and we were able to compute several "views" e.g. an actor and a bimodal graph from the same type of underlying data.

\todo{todo}

\chapter{other things}
report

code

presentation
\end{document}
