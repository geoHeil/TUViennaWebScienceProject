\documentclass[11pt]{report}

\usepackage{todonotes}
\usepackage{hyperref}
\begin{document}
\title{Web Science TU Vienna graph analysis project}
\author{Georg Heiler 1225063 | Jasmina Kadic}
\maketitle
\tableofcontents

\begin{abstract}
	Graph analysis of partial twitter network obtained by hashtag \textbf{trump}. \todo{TODO}
\end{abstract}

\chapter{Data Analysis and visualisation}
<<librariesLoading, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=FALSE, message=FALSE>>=
wants <- c("SocialMediaLab", "magrittr", "igraph", "ggplot2", "wordcloud", "tm", "stringr", "lubridate", "scales", "dplyr", "purrr", "tidyr", "readr", "tidytext")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
lapply(wants, library, character.only=T)
@
\section{Overview}
We chose to analyse a partial network of twitter with the hashtag \textbf{trump}.
The data was collected at two points in time. Once early December, the second time early January. Both times around 18000 tweets were collected (which represent the maximum number of tweets which can be retrieved at once before hitting the API limit).

We chose to use the \hyperref{https://github.com/vosonlab/SocialMediaLab}{socialMediaLab} package to collect the data.
It supports to generate several types of networks for our kind of data

\begin{itemize}
  \item \textbf{bimodal} A user can comment or like a post. Therefor if user $i$ has liked and commented on a post two directed edges are created. The generated network is directed, weighted (\#ofComments), bipartite and contains multiple /parallel edges
  
  \item \textbf{actor} Interactions (mention, reply, retweet) between twitter users show who is interacting with whom in relation to a particular hashtag
  
  \item \textbf{semantic} Nodes represent unique concepts (words) extracted from the set of tweets, edges represent the cooccurence of terms for all observations.
  
\end{itemize}

Ego networks only work well out of the box with Instagram, dynamic networks with facebook. Semantic networks should work for this type of data source, however do not work for our data.

We will compare these 3 networks for some metrics.

\section{Network statistics and communities}

Basic statistics of the network are represented in this section.

Calculate different metrics and measures, e.g., number of vertices, number of edges and their weights, degree distribution (resp. in-degree and out-degree in the directed case), centrality indices, clustering coefficient, reciprocity, shortest paths, network diameter, density, number and size of connected components.

Additionally, communities  and centrality metrics are analyzed.

<<label="0keep_metricsNumberVerticesSlow", results='markup', cache=TRUE>>=
load('tweets.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

And here the computation of the metrics. We define the following function to compute them:

<<label="metricsNumberVertices", results='markup', cache=TRUE>>=
calcStats <- function(graphName){
  # "summary" prints the number of vertices,
  #edges and whether the graph is directed: 
  print("Summary")
  summary(graphName)  
  
  # Number of nodes and edges 
  print(paste("Number of vertices:", vcount(graphName)))
  print(paste("Number of edges:", ecount(graphName)))
  print(paste("Graph is directed", is_directed(graphName)))
  print(paste("Edge density:", edge_density(graphName)))
  print(paste("Average distance:",
              mean_distance(graphName, directed=T)))
  print(paste("Diameter:", diameter(graphName, directed=T)))
  print(paste("Reciprocity: ",
              reciprocity(graphName, ignore.loops =TRUE,
                          mode = c("default", "ratio"))))
  
  # Degree distribution (in-degree,out-degree)
  deg <- igraph::degree(graphName)
  print(paste("Average degree:", mean(deg) ))
  deg_in <- igraph::degree(graphName, mode = "in")
  deg_out <- igraph::degree(graphName, mode = "out")
  print(paste("Average in-degree:",mean(deg_in) ))
  print(paste("Average out-degree:", mean(deg_out)))
  
  print("In-degree centrality (from highest to lowest)")
  print(sort(deg_in, decreasing = T)[1:10])
  print("Out-degree centrality (from highest to lowest)")
  print(sort(deg_out, decreasing = T)[1:10])
  
  # Eigenvector centrality
  eig <- eigen_centrality(graphName, directed=T)$vector
  print("Eigenvector centrality (from highest to lowest)")
  print(sort(eig, decreasing = T)[1:10] )
  
  #  Closeness centrality
  clos_in <- igraph::closeness(graphName, mode = "in") 
  print("In-closeness centrality (from highest to lowest)")
  print(sort(clos_in, decreasing = T)[1:10])
  clos_out <- igraph::closeness(graphName, mode = "out") 
  print("Out-closeness centrality (from highest to lowest)")
  print(sort(clos_out, decreasing = T)[1:10])

  # Betweeness centrality
  betw <- igraph::betweenness(graphName, directed=T) 
  print("Betweeness centrality (from highest to lowest)")
  print(sort(betw, decreasing = T)[1:10])

  # Hubs and Authorities
  hs <- hub_score(graphName)$vector
  print("Hub scores(from highest to lowest)")
  print(sort(hs, decreasing = T)[1:10])

  as <- authority_score(graphName)$vector
  print("Authoritie scores(from highest to lowest)")
  print(sort(as, decreasing = T)[1:10])

  # PageRank
  pr<-page_rank(graphName, algo =
                  c("prpack", "arpack", "power"),
                vids = V(graphName),
              directed = TRUE,
              damping = 0.85,
              personalized = NULL,
              weights = NULL,
              options = NULL)$vector
  print("Page rank (from highest to lowest)")
  print(sort(pr, decreasing = T)[1:10])

  # Number and sizes of connected components
  print(paste("Number of connected components:",
              igraph::components(graphName)$no))
  print("Sizes of connected components:")
  print(summary(igraph::components(graphName)$csize))
  # unfortunately graph will not show up in function
  # qplot(igraph::components(graphName)$csize,
  # geom="histogram", binwidth = 100,
  # main="Sizes of connected components")

  # Transitivity measures the probability
  # that the adjacent vertices of a vertex are connected.
  # This is sometimes also called the clustering coefficient. 
  print("Clustering coefficient (type=local)")
  clcoef <- transitivity(graphName, type = "local")
  print(tail(table(clcoef)))
  print(paste("Clustering coefficient, (type=global):",
              transitivity(graphName, type = "global"))) 
  print(paste("Clustering coefficient, (type=average):",
              transitivity(graphName, type = "average"))) 
}
@

For the actor graph statistics are

<<label="metricsNumberVerticesActor", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

For the bimodal graph statistics are
<<label="metricsNumberVerticesBimodal", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@

The bimodal graph contains a lot more edges ie. is bigger. \todo{summary, TODO compare graphs}

\section{Visualization}
Use different layouts to find and highlight patterns and relationships. To make sense of the data more easily, map the metrics and measures calculated onto visual properties such as size, colour, and opacity.

Gephi was used for the visualization in Figure \ref{fig:plotActor}.
as well as for Figure \ref{fig:plotBimodal}.
We used the degree filter in gephi to filter the graph to a minimal degreee and to remove nodes with very few connections.

\begin{figure}[h]
  \centering
  \caption{Actor graph for trump tweets with edges coded as retweet and mention}
  \includegraphics[width=\textwidth]{slides-figure/g1actor1}
  \label{fig:plotActor}
\end{figure}
\begin{figure}[h]
  \centering
  \caption{Bimodal graph for trump tweets}
  \includegraphics[width=\textwidth]{slides-figure/g1bimodal1}
  \label{fig:plotBimodal}
\end{figure}

\todo{Add description to the plots}

\chapter{Longitudinal}
How did the network change? What dynamics can be
identified? Are there critical events in the period observed?

The same metrics as before are computed now, but for a new dataset collected several weeks later.
loading of the new data
<<label="0keep_longLoading", results='markup', cache=TRUE>>=
load('tweets2.Rdata')
g_twitter_actor <- tweets %>% 
  Create("Actor", writeToFile=FALSE)
g_twitter_bimodal <- tweets %>% 
  Create("Bimodal", writeToFile=FALSE)
@

For the actoor
<<label="actorLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_actor)
@

and  for the bimodal version of the graph
<<label="bimodalLong", results='markup', cache=TRUE>>=
calcStats(g_twitter_bimodal)
@


\todo{summary and comparison with original data}
some summary

\section{ego network analysis}
\todo{add ego network visualization from gephi}

\chapter{Text analysis}
What do the users talk about? Are there any keywords/topics that are particularly important? Does this vary in different groups/communities? Are the discussions controversial?
\todo{compare text analysis per group}

We try to figure out if the real Trump is tweeting or one of his co-workers. Apparently only Trump is using an android device, most of the others are using iOS devices. 
We were following along with \url{http://varianceexplained.org/r/trump-tweets/} and adopted their approach for our data.
You can see the times when a tweet is posted form an android or ios device
<<terms123, echo=FALSE, cache=TRUE, warning=FALSE, results='hide', cache=FALSE, message=FALSE>>=
tweets3 <- read_csv("tweets3.csv")
@
<<label="terms", results='markup', cache=TRUE>>=
tweetsSmall <- tweets3 %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))

tweetsSmall %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")

@

Pictures and links per device type. Compared with the original results from \url{http://varianceexplained.org/r/trump-tweets/} the reported gap between android and iOS has shrunk considerably.

<<label="terms2", results='markup', cache=TRUE>>=
tweet_picture_counts <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  count(source,
        picture = ifelse(str_detect(text, "t.co"),
                         "Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "")
@

Only few quoted tweets are published across devices
<<label="terms3", results='markup', cache=TRUE>>=
tweetsSmall %>%
  count(source,
        quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
  ggplot(aes(source, n, fill = quoted)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "") +
  ggtitle('Whether tweets start with a quotation mark (")')
@

Frequently tweeted words
<<label="terms4", results='markup', cache=TRUE>>=
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweetsSmall %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

tweet_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip()
@

By device type

<<label="terms5", results='markup', cache=TRUE>>=
android_iphone_ratios <- tweet_words %>%
  count(word, source) %>%
  filter(sum(n) >= 5) %>%
  spread(source, n, fill = 0) %>%
  ungroup() %>%
  mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
  mutate(logratio = log2(Android / iPhone)) %>%
  arrange(desc(logratio))

android_iphone_ratios %>%
  group_by(logratio > 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Android / iPhone log ratio") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
@

  - iphone is using more hashtags
  - emotional words e.g. idiot, moron, ... are only common on android devices

As \url{http://varianceexplained.org/r/trump-tweets/} this could be accomplised by a sentiment analysis. But it is already pretty clear that some devices use a very different language.

\todo{description}

The number of tweets over time
<<label="textanalysis1", results='markup', cache=TRUE>>=
tweets <- read.csv("data/trumpTweets.csv")
tweets$created <- ymd_hms(tweets$created)
ggplot(data = tweets, aes(x = created)) +
        geom_histogram(aes(fill = ..count..)) +
        theme(legend.position = "none") +
        xlab("Time") + ylab("Number of tweets") + 
        scale_fill_gradient(low = "midnightblue",
                            high = "aquamarine4")
@

Wordcloud
\todo{filter data more}

<<label="termsx", results='markup', cache=TRUE>>=
wordCorpus <- Corpus(VectorSource(tweets))
wordCorpus <- tm_map(wordCorpus, removePunctuation,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower),lazy=TRUE)

wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"),lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, removeNumbers,lazy=TRUE)
wordCorpus <- tm_map(wordCorpus, stripWhitespace,lazy=TRUE)

tdm <- TermDocumentMatrix(wordCorpus, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")[-(1:4)]

wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
@

\chapter{Discussion and conclusion}
What about the quality of the data? What are the limitations? What questions can be answered and what new insights can be gained by analyzing the structure of the networks? What nodes are in strategic locations? What can you tell about the communities identified? What do you learn by looking at the different networks? Why is it interesting to combine the information from structural and textual data? Interpret and discuss your findings!

The quality of the data is rather good. Without the `SocialMediaLab` package for R it would have been complex to generate the graph from the collected tweets. With  it this was an easy task and we were able to compute several "views" e.g. an actor and a bimodal graph from the same type of underlying data.

The twitter graph data contains a lot of columns
<<label="colnames", results='markup', cache=TRUE>>=
colnames(tweets3)
@
which specify when, what and how e.g. by which device something was tweeted, as well as the type of tweet (mention, retweet).


\todo{todo answer her questions}

\chapter{other things}
report (nearly complete)

code (included in report)

presentation \todo{prepare, I suggest sort of interactive mode}
\end{document}
